Оглавление

[Инструкция по введению в эксплуатацию серверов. 1](#_Toc182279328)

[Часть 1. Приемка сервера и коммутация сервера. 1](#_Toc182279329)

[Этап 1: Приемка сервера. 1](#_Toc182279330)

[Этап 2. Коммутация и включение сервера. 1](#_Toc182279331)

[Часть 2. BIOS, порт удаленного управления, мониторинг 2](#_Toc182279332)

[Этап 1. BIOS. 2](#_Toc182279333)

[Этап 2. Порт удаленного управления. 3](#_Toc182279334)

[Этап 3. Мониторинг 4](#_Toc182279335)

[Установка ОС и подготовка к тестам.. 4](#_Toc182279336)

[Нагрузочное тестирование компонентов. 4](#_Toc182279337)

[Тестирование отказоустойчивости компонентов. 5](#_Toc182279338)

[Отключение и извлечение блока питания: имитация отказа блока питания. 5](#_Toc182279339)

[Извлечение накопителей: имитация отказа накопителей. 6](#_Toc182279340)

[Тестирование сетевых интерфейсов и пропускной способности. 8](#_Toc182279341)

# Инструкция по введению в эксплуатацию серверов

Для введения серверов в эксплуатацию и использования их в продакшене требуется выполнить обязательный порядок действий. Эти шаги являются обязательными и требуют внимания.

Для каждого сервера требуется заполнить чек-лист по пунктам инструкции.  Сервер помечается как испорченный, если хотя бы один из критичных критериев не проходит проверку

## Часть 1. Приемка сервера и коммутация сервера

Приемка сервера является критичным критерием.  
При поступлении сервера требуется провести:

### Этап 1: Приемка сервера

1.      Проведение внешнего осмотра:

1.1.   Осмотрите корпус сервера на наличие видимых повреждений (вмятин, царапин, сколов).

1.2.   Проверьте кабели, блоки питания, кнопки и разъемы на наличие повреждений и целостность креплений.

1.3.   Убедитесь, что на корпусе и внутренних компонентах нет признаков ударов, трещин или коррозии.

2.      Проверка внутренних компонентов:

2.1.   Откройте корпус и осмотрите плату, процессоры, модули памяти, накопители и кабели.

2.2.   Проверьте крепление всех модулей, особенно оперативной памяти и процессора: они должны быть надежно зафиксированы.

2.3.   Проверьте вентиляторы – они должны быть установлены плотно и вращаться без заеданий.

2.4.   Если сервер прибыл в разобранном виде компоненты проверяются ДО и ПОСЛЕ установки в сервер.

3.      Устранение неполадок (если обнаружены):

3.1.   В случае обнаружения повреждений, зафиксируйте их и сделайте фотографии.

3.2.   Оповестите руководство или службу поддержки, если повреждения серьезные или присутствуют недостающие компоненты.

4.      Сборка сервера.

4.1.   Сборка сервера осуществляется, если сервер поступил в разобранном виде.

4.2.   Компоненты сервера являются модульными и подключаются согласно документации.

4.3.   Сборка осуществляется аккуратно, без применения силы.

Этап 2. Коммутация и включение сервера.  
ЗАПРЕЩАЕТСЯ УСТАНАВЛИВАТЬ СЕРВЕР В СТОЙКУ С ПРОДАКШН СЕРВЕРАМИ.

После осмотра и сборки сервера необходимо установить его в стойку и скоммутировать для тестирования.

1.      Установка сервера в стойку:

1.1.   Установите сервер в стойку, используя рекомендованные производителем монтажные крепления или шасси, чтобы обеспечить надежность крепления.

2.      Подключение сетевых и периферийных устройств:

2.1.   Подключите кабели Ethernet к сетевым портам сервера. Обычно подключают два порта: основной и резервный.

2.2.   Подключите кабель к порту удаленного управления (ILO/DRAC/IPMI) для удаленного контроля и диагностики.

2.3.   Подключите PSU используя кабели питания, подключив их к PDU (подключение производится симметрично, подключение PSU производится в разные PDU – один основной луч, другой резервный).

2.4.   Подсоедините монитор, клавиатуру и мышь к серверу для доступа к начальной настройке.

3.      Включение сервера

3.1.   Произвести включение сервера нажатием на кнопку на передней панели

3.2.   Проверьте, что все индикаторы на компонентах (диски, питание, сеть) работают нормально.

3.3.   Ожидайте завершения POST (Power-On Self Test). Если система не проходит POST, зафиксируйте код ошибки и устраните причину.

3.4.   Выключаем сервер

Часть 2. BIOS, порт удаленного управления, мониторинг

### Этап 1. BIOS

1.      Вход в BIOS:

1.1.   Нажмите необходимую клавишу (обычно F2, F10, Delete) при загрузке сервера, чтобы войти в BIOS.

2.      Проверка конфигурации:

2.1.   Убедитесь, что BIOS корректно распознает процессоры, модули памяти и диски.

2.2.   Проверьте наличие и корректность кода версии BIOS и BMC.

3.      Обновление прошивок:

3.1.   Если необходимо, обновите BIOS и прошивки BMC. Используйте только версии, рекомендованные производителем.

3.2.   Проверьте наличие обновлений для RAID-контроллера и PCIe-карт. Убедитесь, что обновление завершилось без ошибок.

4.      Настройка базовых параметров:

4.1.   Настройте системное время и дату для синхронизации с сервером мониторинга. Если для времени используется NTP сервер, то по возможности использовать его.

4.2.   Включите опции виртуализации и Hyper-Threading (если необходимо для задач сервера).

4.3.   Включаем параметр для сервера включаться всегда, когда есть питание.

4.4.   Настройте последовательность загрузки, указав на USB в приоритете для загрузки установщика тестовой ОС.

4.5.   Настройте сетевые параметры (ip, маска, шлюз) для порта удаленного управления (ILO/DRAC/IPMI) и, по возможности, учетную запись администратора (логин и пароль)

5.      Настройка загрузки в режиме UEFI:

5.1.   Включите режим загрузки UEFI, если система поддерживает его и планируется установка UEFI-совместимой ОС.

5.2.   Если совместимость с Legacy обязательна для некоторых компонентов, включите режим Dual Boot или Legacy.

### Этап 2. Порт удаленного управления.

1.      Подключение к порту управления (ILO, DRAC, IPMI):

1.1.   Проверьте сетевую связность с портом управления.

1.2.   Проверьте что по указанному адресу порта управления есть доступ через браузер и ssh.

1.3.   Войдите в систему удаленного управления (ILO, DRAC, IPMI) через браузер и ssh с настроенными ранее логином и паролем. В иных случаях используйте логин и пароль указанные производителем.

2.      Проверка аппаратной конфигурации и состояния компонентов:

2.1.   Убедитесь, что на экране управления отображаются корректные сведения о сервере: серийный номер, модель, версия микрокода BIOS и BMC.

2.2.   Проверьте статус установленных компонентов (процессор, ОЗУ, накопители, модули PCIe).

2.3.   Проверьте показания датчиков температуры, напряжения, частоты вращения вентиляторов. Если есть возможность настройте пороговые значения для данных датчиков.

2.4.   Просмотрите журналы событий BMC, чтобы убедиться в отсутствии ошибок, связанных с оборудованием.

3.      Проверка функциональных возможностей системы удаленного управления (ILO, DRAC, IPMI):

3.1.   Протестируйте функцию управления включением, выключением, перезагрузкой сервера

3.2.   Создание новых пользователей и групп с определенными правами доступа (администратор, оператор, только чтение).

3.3.   Использование HTML KVM-консоли или Java-консоли для установки и управления ОС.

3.4.   Обновление версии микрокода BMC без влияния на работу ОС.

3.5.   Генерация нового или добавление существующего SSL-сертификата.

3.6.   Экспорт и импорт конфигурации интерфейса управления.

3.7.   Сбросом порта управления до заводских настроек;

3.8.   Отправка сообщений о событиях с использованием SMTP, syslog и SNMP Trap.

Дополнительные функциональные возможности можно узнать из документации к серверу от производителя.

### Этап 3. Мониторинг

Перед проведением тестов компонентов нам требуется настроить мониторинг сервера. От мониторинга мы получим статистку и проверим работоспособность мониторинга со стороны сервера.

1.      Импорт и настройка MIB-файлов:

1.1.   Загрузите и импортируйте MIB-файлы производителя в систему мониторинга (например, Zabbix, Nagios).

2.      Создание шаблонов мониторинга:

2.1.   Настройте шаблоны для мониторинга температур, энергопотребления, состояния компонентов (процессор, ОЗУ, диски).

3.      Настройка оповещений:

3.1.   Задайте пороговые значения для предупреждений и оповещений.

3.2.   Убедитесь, что уведомления отправляются при перегрузке процессора, перегреве или отказе дисков.

## Часть 3. Нагрузочное тестирование.

### Этап 1. Установка ОС и подготовка к тестам

Закончив с предварительными настройками, мы можем переходить к установке ОС и запуске нагрузочного тестирования сервера.

1.      Подключите носитель с установочным образом к серверу

2.      Создайте RAID массив (если сервер содержит RAID контроллер)

2.1.  Уровень RAID выбирается в зависимости от количества дисков и задач сервера

2.2.  В основном используются уровни RAID 1 или 10, но также допускается уровень RAID5

3.      Установка ОС

3.1.             Для установки ОС используется дистрибутив Linux, который поддерживает утилиты из списка далее.

3.2.             В основном используются дистрибутивы Ubuntu и Debian

3.3.             Произведите установку ОС. При установке используйте LVM и выделите под корневой раздел все доступное пространство. Создайте файловую систему.

3.4.             Настройте тестовую сеть

4.      После установки

4.1.             Обновите список пакетов и скачайте, и установите обновления системы.

4.2.             Установите необходимые для тестирования утилиты:

·                  **badblocks** – для проверки дисков на наличие поврежденных секторов.

·                  **stress-ng** – для создания нагрузки на процессор и оперативную память.

·                  **fio** – для тестирования производительности дисковой подсистемы.

·                  **dd** – для измерения скорости записи данных на диск.

·                  **htop** – для мониторинга загрузки процессора, памяти и других системных ресурсов в реальном времени.

·                  **smartmontools** (включает smartctl) – для проверки состояния жестких дисков и SSD, поддержки S.M.A.R.T.

·                  **iperf****3** – Для тестирования пропускной способности сети

## Нагрузочное тестирование компонентов

После всех подготовок можно приступать к тестированию.

1)      Процессор, память и температура.

Для проверки работы процессора, оперативной памяти и охлаждения запускаем стресс тест через утилиту stress-ng. Данный тест загружает каждый поток процессора на максимум и 98% оперативной памяти. Процент памяти надо указывать такой, чтобы осталось памяти для работы операционной системы. Иначе операционная система отключит выполнение тестов. Во время теста следим за ОС и сервером через утилиту htop. Смотрим что бы система не зависала и утилизировала указанные ресурсы. Сервер все 2 часа должен проработать без перезагрузок. Так же наблюдаем за сервером через менджмент порт. Смотрим температуру системы и отчеты в журналах (нагрев, перезагрузки и тд). Записываем показатель bogo ops/s (real time) и температуры.

**stress-ng --cpu 0 --cache 0 -m 0 --vm-bytes (объем RAM в сервере в %) --io 0 --hdd 0 -t (время теста) s --metrics-brief --tz**

2)      Дисковая подсистема.

Перед запуском тестов проверим здоровье дисков. Для этого используем утилиту smartmontools

Так как диски находятся в RAID, то наша система видит его как логический диск. Для того что бы определить физические диски просканируем систему.

**smartctl --scan**

Данная команда отразит рейд контроллер и номер канала. Эти данные мы используем в проверке

**smartctl -****H -****d** **megaraid,5 (рейд контроллер и номер канала из сканирования) /****dev/****sda**

Вывод будет формата ок или не ок.

Для тестов дисковой подсистемы мы используем утилиты badblock, fio, dd  
badblock проверяет блоки жесткого диска.  
**badblocks -v****s /dev/sda (указываем диск или раздел который хотим протестировать)**

После проверки блоков если все блоки целые проверяем скорость чтения/записи. Пишем временный файл, состоящий из блоков размером в 1m количеством 40960 скидывая кэш. Скорость записи должна быть большой так как файл сначала пишется в кэш потом на диск

**dd status=progress if=/dev/zero of=/tmp/tempfile bs=1M count=40960 oflag=sync**

**sync; dd if=/dev/zero of=/tmp/tempfile bs=1M count=1024; sync**

Прогоняем данный тест 10 раз. Скорость не должна опускаться меньше 90 MB/s.

## Тестирование отказоустойчивости компонентов

После нагрузочного тестирования требуется протестировать отработку отказа компонентов с горячей заменой. А именно блоки питания и диски в RAID. Для данного тестирования мы поочередно отключаем компоненты, а потом возвращаем обратно. Во время тестов смотрим на работоспособность системы и отчеты в журнале менеджмент порта.

## Отключение и извлечение блока питания: имитация отказа блока питания

Этапы:

1.      Перед проведением тестирования для получения эталонных результатов запускаем утилиту Stress-ng на 2 часа с параметрами, которые были описаны в разделе «Нагрузочное тестирование процессора». Фиксируем результаты для последующего сравнения.

2.      Для создания нагрузки на сервер при выполнении тестирования повторно запускаем утилиту Stress-ng на 2 часа с параметрами, описанными в разделе «Утилита Stress-ng».

3.      От блока питания PSU1 отключаем кабель питания и наблюдаем за BMC сервера с использованием IPMI на предмет появления уведомлений об ошибках и создания записей в системном журнале.

4.      Блок питания PSU1 извлекаем из сервера и наблюдаем за BMC сервера, как описано в пункте 3.

5.      По истечении 2 часов работы сервера под нагрузкой на одном блоке питания PSU2 фиксируем полученные результаты работы утилиты Stress-ng, чтобы сравнить с результатами, полученными при выполнении пункта 1.

6.      Устанавливаем обратно блок питания PSU1. Наблюдаем за BMC сервера, как описано в пункте 3.

7.      К блоку питания PSU1 подключаем кабель питания и наблюдаем за BMC сервера, как описано в пункте 3.

8.      Повторно выполняем пункты со 2 по 7 включительно для блока питания PSU2.

Критерии успешного прохождения теста

·        Во время работы под нагрузкой сервер доступен по ssh.

·        При отключении от блока питания в BMC появилось уведомление, в системном журнале создалась запись о событии, датчики зафиксировали отсутствие напряжения на блоке питания.

·        При извлечении блока питания из сервера в BMC создалось уведомление, определяется только один блок питания.

·        При возврате блока питания в сервер и подключении кабеля питания в BMC определяются все блоки питания, все датчики с нормальными значениями, в системном журнале создалась запись о наличии напряжения на блоке питания.

·        Результаты утилиты Stress-ng при работе сервера под нагрузкой и на одном блоке питания не отличаются в меньшую сторону больше чем на 10% по сравнению с результатами, полученными при работе сервера под нагрузкой на двух блоках питания.

## Извлечение накопителей: имитация отказа накопителей

Этапы

На свободных накопителях в сервере создаем дисковое хранилище с уровнем резервирования RAID1, RAID6 или RAID10 через BMC, BIOS или с использованием, например, утилиты StorCLI. Тестирование проводим для каждого уровня резервирования дискового хранилища.

При возможности один из свободных накопителей настраиваем как резервный (Global Hot Spare). Зависит от количества дисков и уровня RAID

Для дискового хранилища без файловой системы

1.      Запускаем утилиту FIO с профилем нагрузки: 40% случайных операций записи и 60% случайных операций чтения блоком 8 КБ, 16 потоков. Фиксируем производительность в IOPS на чтение и запись.

2.      Извлекаем из сервера один из накопителей. До начала перестроения дискового хранилища снова фиксируем производительность в IOPS на чтение и запись. Наблюдаем за BMC сервера на предмет появления уведомлений и создания записей в системном журнале.

3.      Контролируем статус и процесс перестроения (rebuild) дискового хранилища, если есть свободный резервный накопитель в сервере (Global Hot Spare). Фиксируем производительность в IOPS на чтение и запись во время перестроения дискового хранилища.

4.      При использовании дискового хранилища с уровнем резервирования RAID6 или RAID10 извлекаем второй накопитель из сервера. Фиксируем производительность в IOPS на чтение и запись.

5.      По завершении перестроения дискового хранилища все ранее извлеченные накопители устанавливаем обратно. Отслеживаем статус всех накопителей, а также наблюдаем за BMC сервера, как описано в пункте 2.

6.      При необходимости через BMC или с помощью утилиты StorCLI в ОС удаляем стороннюю конфигурацию на накопителях и меняем их статусы.

7.      По завершении второго перестроения дискового хранилища отслеживаем статус и состав дискового хранилища, а также статус самих накопителей.

Для дискового хранилища с файловой системой:

1.      На дисковом хранилище создаем один раздел максимального размера и файловую систему ext4, которую подключаем к директории test1..

2.      Запускаем копирование большого файла (несколько десятков ГБ) в директорию test1.

3.      Извлекаем из сервера один из накопителей. Отслеживаем статус дискового хранилища и наблюдаем за BMC сервера на предмет появления уведомлений и создания записей в системном журнале.

4.      Контролируем процесс перестроения дискового хранилища, если есть свободный накопитель в сервере (Global Hot Spare), и отслеживаем скорость копирования файла.

5.      Если используем дисковое хранилище с уровнем резервирования RAID6 или RAID10, извлекаем второй накопитель из сервера.

6.      После перестроения дискового хранилища и окончания копирования файла устанавливаем накопители обратно. Отслеживаем статус всех накопителей, а также наблюдаем за BMC сервера, как описано в пункте 3.

7.      Вычисляем и сравниваем контрольные суммы файла-источника и скопированного файла.

8.      При необходимости через BMC или с помощью утилиты StorCLI в ОС удаляем стороннюю конфигурацию на установленных накопителях с последующим изменением статусов установленных накопителей.

9.      После второго перестроения дискового хранилища отслеживаем статус и состав дискового хранилища, а также статус самих накопителей.

Критерии выполнения теста

Дисковое хранилище без файловой системы

·        Работа утилиты FIO не прерывалась во время тестирования, и дисковое хранилище было постоянно доступно на чтение и запись.

·        Сработала автоматическая замена извлеченного накопителя на резервный (Global Hot Spare).

·        Дисковое хранилище успешно выполнило все перестроения.

·        Возврат к начальной конфигурации дискового хранилища (состав и статус накопителей) происходит без перезагрузки сервера.

Дисковое хранилище с файловой системой

·        Файловая система на дисковом хранилище была доступна на протяжении всего времени тестирования.

·        Копирование файла завершилось без прерываний.

·        Сработала автоматическая замена извлеченного накопителя на резервный (Global Hot Spare).

·        Дисковое хранилище успешно выполнило все перестроения.

·        Контрольные суммы файла-источника и скопированного файла остались одинаковыми.

·        Конфигурация дискового хранилища (состав и статус накопителей) вернулась в начальное состояние без перезагрузки сервера.

## Тестирование сетевых интерфейсов и пропускной способности

Для тестирования сетевых интерфейсов на потребуется приемник трафика. Приемник должен обладать достаточной пропускной способностью и иметь возможность запускать утилиту iperf3. То есть для интерфейса скоростью 10 Гбит\с у приемника должен быть такой же по скорости интерфейс.

При подключении интерфейса должна загораться индикация на нем.  На интерфейсах и приемнике настраиваем ip адреса. Тестируемый интерфейс и приемник должны обладать сетевой связностью. Пропингуем интерфейс с приёмника. Он должен быть доступен и передавать пакеты без потерь.  
 

Для тестирования пропускной способности сети воспользуемся утилитой iperf3.

На приемнике запускаем утилиту iperf3 –s

-s Запуск в режиме сервера. В данном режиме машина принимает трафик по порту 5201

На тестируемом сервере так же запускаем утилиту iperf3

iperf3 -c (ip адрес приемника) -t 1800 -f g -P 20

-с Запуск в режиме клиента с указанием ip адреса сервера

-t Время работы утилиты

-f формат вывода пропускной способности k, m, g килобиты, мегабиты, гигабиты. K, M, G килобайты, мегабайты, гигабайты

-P Количество потоков генерации трафика.

Значение ключа –P требуется подобрать такое что бы утилизация интерфеса была максимальная.

Критерии выполнения теста:

·        Сетевые интерфейсы включатся и имеют индикацию

·        Между приемником и передатчиком ходят пакеты

·        Пропускная способность интерфейса во время теста больше 90 % от заявленной производителем